from Auxiliares.requirements import *
from Auxiliares.auxiliar_functions import *
from Algoritmos.AE_PUL import autoencoder_PUL_model
from torch_geometric.nn import GCNConv

def mst_graph(X):
    """Returns Minimum Spanning Tree (MST) graph from the feature matrix.

    Parameters
    ----------
    X : ndarray, shape (N, F)
        N samples and F-dimensional features.

    Returns
    -------
    adj : ndarray, shape (N, N)
        The adjacency matrix of the constructed mst graph.
    """
    D = euclidean_distances(X, X)
    adj_directed = minimum_spanning_tree(D).toarray()
    adj = adj_directed + adj_directed.T
    adj[adj > 0] = 1
    np.fill_diagonal(adj,0)

    return csr_matrix(adj)

def positive_weighted_path(X, G, P):
    '''
    P: list of positive nodes
    G: networkx graph
    X: feature matrix

    If the graph is not connected, the function mst_graph(X) should be called to connect the graph, ie, if the adjacency matrix of node u,v in MST is 1 and in adjacency of G is 0, then  G[u][v] = 1.

    return: the weighted path between all pairs of positive nodes, for each pair of positive nodes, the shortest path is considered
    '''
    n = len(G.nodes())
    A = np.zeros((n,n))

    # verify if G is connected, if not, then connect it using mst_graph function. compute MST of G, if the adjacency matrix of node u,v in MST is 1 and in G is 0, then  G[u][v] = 1.
    if not nx.is_connected(G):
        A = mst_graph(X).toarray()
        adj = nx.adjacency_matrix(G).toarray()
        # for every u,v in A, if A[u,v] == 1 and adj[u,v] == 0 then, adj[u,v] = 1
        for i in range(n):
            for j in range(n):
                if A[i][j] == 1 and adj[i][j] == 0:
                    adj[i][j] = 1
        adj = np.matrix(adj)
        _G = nx.DiGraph()
        num_nodes = len(adj)
        for node in range(num_nodes):
            G.add_node(node)  # Add nodes to the graph
        for i in range(num_nodes):
            for j in range(num_nodes):
                if adj[i, j] == 1:
                    _G.add_edge(i, j)
        _G = _G.to_undirected()
    # compute the shortest path between all pairs of positive nodes using _G
    for i in range(len(P)):
        for j in range(i+1, len(P)):
            path = nx.shortest_path(_G, source = P[i], target = P[j])
            for k in range(len(path)-1):
                A[path[k]][path[k+1]] += 1
                A[path[k+1]][path[k]] += 1
    print('Graph is connected? ', nx.is_connected(_G))
    return torch.tensor(A)

def connect_positive_nodes(G, P):
    '''
    Connect all positive nodes in the graph

    return: returns the adjacency matrix of the graph with all positive nodes connected.
    '''

    for i in range(len(P)):
        for j in range(i+1, len(P)):
            G.add_edge(P[i], P[j])
    return torch.tensor(nx.adjacency_matrix(G).toarray()).double()


dataset = Planetoid(root = 'Datasets', name = "Cora", transform=NormalizeFeatures())
data = dataset[0]

G = to_networkx(data, to_undirected=True)
adj = nx.adjacency_matrix(G).toarray()
X = data.x.double()
Y = data.y
# CORA
pul_label = [0,1,2,4]
# CiteSeer
# pul_label = [2,3,4]

Y = torch.tensor([1 if x in pul_label else 0 for x in Y])

all_positives = [index for index in range(len(Y)) if Y[index] == 1]
all_negatives = [index for index in range(len(Y)) if Y[index] == 0]

A = torch.tensor(nx.adjacency_matrix(G).todense())
D = degree_matrix(G)
A_tilde = A + torch.eye(len(G.nodes()))
A_tilde = A_tilde.double()
D_tilde = inverse_sqroot_matrix(D)


class Autoencoder(nn.Module):
    def __init__(self, input_size, hidden_size1, hidden_size2):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size1),
            nn.ReLU(),
            nn.Linear(hidden_size1, hidden_size2),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_size2, hidden_size1),
            nn.ReLU(),
            nn.Linear(hidden_size1, input_size),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
    
# Primeira hipótese para o GAE - Regularização utilizando a MST para conectar o grafo e calculando os caminhos mais curtos entre os pares de nós positivos.
class Regularized_GAE(torch.nn.Module):
    def __init__(self, in_channel, hid_channel1, hid_channel2, D_tilde, C):
        super(Regularized_GAE, self).__init__()
        self.D_tilde = D_tilde
        self.C = C
        self.encoder = nn.Sequential(
            nn.Linear(in_channel, hid_channel1),
            nn.ReLU(),
            nn.Linear(hid_channel1, hid_channel2),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(hid_channel2, hid_channel1),
            nn.ReLU(),
            nn.Linear(hid_channel1, in_channel),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.encoder(x)
        # Message Passing considering the shortest path of a pair of positive nodes
        x = torch.matmul(torch.matmul(torch.matmul(self.D_tilde, self.C), self.D_tilde), x)
        x = F.relu(x)
        x = torch.matmul(torch.matmul(torch.matmul(self.D_tilde, self.C), self.D_tilde), x)
        x = F.relu(x)
        # x = torch.matmul(torch.matmul(torch.matmul(self.D_tilde, self.C), self.D_tilde), x)
        # x = F.relu(x)
        x = self.decoder(x)
        return x


class Regularized_GAE2(torch.nn.Module):
    def __init__(self, in_channel, hid_channel1, hid_channel2):
        super(Regularized_GAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(in_channel, hid_channel1),
            nn.ReLU(),
            nn.Linear(hid_channel1, hid_channel2),
            nn.ReLU()
        )
        self.conv1 = GCNConv(hid_channel2, hid_channel2)
        self.decoder = nn.Sequential(
            nn.Linear(hid_channel2, hid_channel1),
            nn.ReLU(),
            nn.Linear(hid_channel1, in_channel),
            nn.Sigmoid()
        )    

    def forward(self, x, edge_index):
        x = self.encoder(x)
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.decoder(x)
        return x



epochs = [1,2,3,4, 5, 10]


positive_rate = 0.05
for i in range(10):
    positives = random.sample(all_positives, int(positive_rate * len(all_positives)))
    unlabeled = list(set(range(len(G.nodes()))) - set(positives))   
    print('aaaaaaaaaaaaaaaa')
    # C = positive_weighted_path(X, G, positives)
    # C = torch.sqrt(C)

    #C = connect_positive_nodes(G, positives)

    model1 = Regularized_GAE2(in_channel = X.shape[1], hid_channel1=256, hid_channel2=128, D_tilde = D_tilde, C = C)
    model2 = Autoencoder(input_size = X.shape[1], hidden_size1 = 256, hidden_size2 = 128)
    optimizer1 = torch.optim.Adam(model1.parameters(), lr = 0.01)
    optimizer2 = torch.optim.Adam(model2.parameters(), lr = 0.01)

    print(len(positives))
    for epoch in epochs:
        GAE_classifier = autoencoder_PUL_model(model = model1, optimizer = optimizer1, epochs = epoch, data = X, positives = positives, unlabeled = unlabeled)
        GAE_classifier.train()
        RN_GAE = GAE_classifier.negative_inference(num_neg = 200)
        print(f'GAE: quantidade de epocas de treinamento {epoch} \t acurácia {compute_accuracy(Y, RN_GAE)}')
        model1 = Regularized_GAE(in_channel = X.shape[1], hid_channel1=256, hid_channel2=128, D_tilde = D_tilde, C = C)
        optimizer1 = torch.optim.Adam(model1.parameters(), lr = 0.001)

        AE_classifier = autoencoder_PUL_model(model = model2, optimizer = optimizer2, epochs = epoch, data = X, positives = positives, unlabeled = unlabeled)
        AE_classifier.train()
        RN_AE = AE_classifier.negative_inference(num_neg = 200)
        print(f'AE : quantidade de epocas de treinamento {epoch} \t acurácia {compute_accuracy(Y, RN_AE)}')
        model2 = Autoencoder(input_size = X.shape[1], hidden_size1 = 256, hidden_size2 = 128)
        optimizer2 = torch.optim.Adam(model2.parameters(), lr = 0.01)


